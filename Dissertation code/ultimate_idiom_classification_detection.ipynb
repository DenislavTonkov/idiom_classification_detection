{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T20:35:27.371827Z",
     "start_time": "2022-05-03T20:35:26.638464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('Lstm_model')\n",
    "#Loading the pre-trained LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T20:35:27.483134Z",
     "start_time": "2022-05-03T20:35:27.373711Z"
    }
   },
   "outputs": [],
   "source": [
    "idiom=pd.read_excel('idiom.xlsx')\n",
    "#Loading the dataset, which is just the idioms column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T20:51:54.431363Z",
     "start_time": "2022-05-03T20:51:54.409363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idioms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>off the beaten track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>give someone the creeps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do someone proud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>there's no fool like an old fool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>top banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>all dressed up and nowhere to go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>keep your nose clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>ring off the hook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1738 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                idioms\n",
       "0                 off the beaten track\n",
       "1                       in the running\n",
       "2              give someone the creeps\n",
       "3                     do someone proud\n",
       "4                            take root\n",
       "...                                ...\n",
       "1733  there's no fool like an old fool\n",
       "1734                        top banana\n",
       "1735  all dressed up and nowhere to go\n",
       "1736              keep your nose clean\n",
       "1737                 ring off the hook\n",
       "\n",
       "[1738 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idiom\n",
    "#Checking/displaying the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:40.662430Z",
     "start_time": "2022-05-03T21:12:38.834892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the trees have taken rootthe trees take root\n"
     ]
    }
   ],
   "source": [
    "text=input(\"the trees have taken root\")\n",
    "#User input to test the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:45.533234Z",
     "start_time": "2022-05-03T21:12:45.437055Z"
    }
   },
   "outputs": [],
   "source": [
    "idiom_temp=''\n",
    "for i in range(idiom.shape[0]):\n",
    "    if idiom.idioms[i] in text:\n",
    "        idiom_temp=idiom.idioms[i]\n",
    "#Detecting in the user input if there is a present idiom, !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:15:52.403333Z",
     "start_time": "2022-05-03T21:15:52.387330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'take root'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idiom_temp\n",
    "#Printing out if there is a present idiom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:15:45.416937Z",
     "start_time": "2022-05-03T21:15:45.401272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential idiom\n"
     ]
    }
   ],
   "source": [
    "if idiom_temp=='':\n",
    "    print('Potenial non idiom')\n",
    "else:\n",
    "    print('Potential idiom')\n",
    "#Console output of classification of the potential idiom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:48.298524Z",
     "start_time": "2022-05-03T21:12:45.553231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timeit\n",
    "from transformers import BertTokenizer, BertModel\n",
    "model1 = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing all the necessary libraries to run the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:50.417209Z",
     "start_time": "2022-05-03T21:12:48.301512Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initialisting and running the BERT Model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "recor=[]\n",
    "# text='off the beaten track'\n",
    "\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text) #\n",
    "if len(tokenized_text)>512:\n",
    "    tokenized_text=tokenized_text[:511]\n",
    "    tokenized_text.append(\" [SEP]\")\n",
    "\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "#         for tup in zip(tokenized_text, indexed_tokens):\n",
    "#             print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "\n",
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "#         print (segments_ids)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "\n",
    "\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model1.eval()\n",
    "\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model1(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "#         print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "#         print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "#         print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "#         print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
    "\n",
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = hidden_states[layer_i][batch_i][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "# plt.figure(figsize=(10,10))\n",
    "#  plt.hist(vec, bins=200)\n",
    "# plt.show()\n",
    "\n",
    "# `hidden_states` is a Python list.\n",
    "#         print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "#         print('Tensor shape for each layer: ', hidden_states[0].size())\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()\n",
    "\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "token_embeddings.size()\n",
    "\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "#         print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "#         print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\n",
    "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "#         print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "\n",
    "#         for i, token_str in enumerate(tokenized_text):\n",
    "#           print (i, token_str)\n",
    "    \n",
    "\n",
    "\n",
    "try:\n",
    "    o=len(idiom_temp.split())\n",
    "    p=0\n",
    "    for i in range(len(tokenized_text)-o+1):\n",
    "        if ' '.join(tokenized_text[i:i+o])==idiom_temp:\n",
    "            p=i\n",
    "            break\n",
    "        else:\n",
    "            p='error'\n",
    "    str(tokenized_text[i])\n",
    "    tokenized_text[p],tokenized_text[p+o-1]\n",
    "    k=[]\n",
    "    for i in range(p,p+o,+1):\n",
    "        k.append(token_vecs_sum[i][:5].tolist()) \n",
    "        #Taking the first 5 layers out of the 769 as they contain the most amount of information, relevant to classification in my opinion.\n",
    "    k=reduce(lambda z, y :z + y, k)\n",
    "#         mean_tensor=(float(sum(list(token_vecs_sum[i][:5])))+float(sum(list(token_vecs_sum[i+1][:5])))+float(sum(list(token_vecs_sum[i+2][:5]))))/3\n",
    "    recor.append(k)\n",
    "except:\n",
    "    recor.append('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:50.432879Z",
     "start_time": "2022-05-03T21:12:50.417209Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(45-len(k)):\n",
    "    k.append(0)\n",
    "#The idiom can be a maximum of 9 words and the if we have 5 layers per word, the max layers we could take would be 9 * 5 = 45.\n",
    "#So if we have an idiom of length 20 words for example, the layers after the 45-th \n",
    "#would be nothing so we just add zeros so that the LSTM model doesn't cause an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:50.448879Z",
     "start_time": "2022-05-03T21:12:50.435882Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test=np.array([k])\n",
    "x_test=np.reshape(x_test,(x_test.shape[0],1,x_test.shape[1]))\n",
    "#Reshaping as per the LSTM Model input standard requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:50.528876Z",
     "start_time": "2022-05-03T21:12:50.452879Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "#The prediction function we invoke from the LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:50.544879Z",
     "start_time": "2022-05-03T21:12:50.532879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01160494]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred#variable that shows how idiomatic or literal the sentece is. By rounding we get a definitive answer of whether it is an idiom or literal!\n",
    "#Printing out the prediction, which can be a number from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T21:12:50.560881Z",
     "start_time": "2022-05-03T21:12:50.549883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is likely to be contextually idiomatic.\n"
     ]
    }
   ],
   "source": [
    "if round(y_pred[0][0])==0:\n",
    "    print('The input is likely to be contextually idiomatic.')\n",
    "elif round(y_pred[0][0])==1:\n",
    "    print('The input is likely contextually literal.')\n",
    "#Final classification/prediction, where if the number in y_pred can be rounded to either 2 extremes, it is \n",
    "#and we coclude from that whether the input is idiomatic or literal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
